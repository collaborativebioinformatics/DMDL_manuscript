
@misc{wang_cord-19_2020,
	title = {{CORD}-19: {The} {COVID}-19 {Open} {Research} {Dataset}},
	shorttitle = {{CORD}-19},
	url = {http://arxiv.org/abs/2004.10706},
	abstract = {The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Wang, Lucy Lu and Lo, Kyle and Chandrasekhar, Yoganand and Reas, Russell and Yang, Jiangjiang and Burdick, Doug and Eide, Darrin and Funk, Kathryn and Katsis, Yannis and Kinney, Rodney and Li, Yunyao and Liu, Ziyang and Merrill, William and Mooney, Paul and Murdick, Dewey and Rishi, Devvret and Sheehan, Jerry and Shen, Zhihong and Stilson, Brandon and Wade, Alex and Wang, Kuansan and Wang, Nancy Xin Ru and Wilhelm, Chris and Xie, Boya and Raymond, Douglas and Weld, Daniel S. and Etzioni, Oren and Kohlmeier, Sebastian},
	month = jul,
	year = {2020},
	note = {arXiv:2004.10706 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Digital Libraries},
	annote = {Comment: ACL NLP-COVID Workshop 2020},
	file = {arXiv Fulltext PDF:/Users/jedrzejkubica/Zotero/storage/F75N64ID/Wang et al. - 2020 - CORD-19 The COVID-19 Open Research Dataset.pdf:application/pdf;arXiv.org Snapshot:/Users/jedrzejkubica/Zotero/storage/AYVUPPQL/2004.html:text/html},
}

@misc{noauthor_httpswwwpineconeiolearnvector-database_nodate,
	title = {https://www.pinecone.io/learn/vector-database/},
}

@misc{cohan_specter_2020,
	title = {{SPECTER}: {Document}-level {Representation} {Learning} using {Citation}-informed {Transformers}},
	shorttitle = {{SPECTER}},
	url = {http://arxiv.org/abs/2004.07180},
	abstract = {Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Cohan, Arman and Feldman, Sergey and Beltagy, Iz and Downey, Doug and Weld, Daniel S.},
	month = may,
	year = {2020},
	note = {arXiv:2004.07180 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2020},
	file = {arXiv Fulltext PDF:/Users/jedrzejkubica/Zotero/storage/BP2CW6FE/Cohan et al. - 2020 - SPECTER Document-level Representation Learning us.pdf:application/pdf;arXiv.org Snapshot:/Users/jedrzejkubica/Zotero/storage/6MRFN6IM/2004.html:text/html},
}

@misc{noauthor_httppeopleeedukeedulcarinp93pdf_nodate,
	title = {http://people.ee.duke.edu/{\textasciitilde}lcarin/p93.pdf},
}

@misc{noauthor_httpstowardsdatasciencecomrandom-projection--python-705883a19e48_nodate,
	title = {https://towardsdatascience.com/random-projection-in-python-705883a19e48},
}

@misc{noauthor_httpsscikit-learnorgstablemodulesgeneratesklearnrandom_projectiongaussianrandomprojectionhtml_nodate,
	title = {https://scikit-learn.org/stable/modules/generate/sklearn.random\_projection.{GaussianRandomProjection}.html},
}

@misc{noauthor_httpslearnmicrosoftcomen-ussemantic-kernelmemoriesvector-db_nodate,
	title = {https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db},
}

@misc{noauthor_httpswwwlinkedincompulsevector-databases-demystified-part-2-building-your-own-adie-kaye_nodate,
	title = {https://www.linkedin.com/pulse/vector-databases-demystified-part-2-building-your-own-adie-kaye/},
}

@article{thirunavukarasu_large_2023,
	title = {Large language models in medicine},
	volume = {29},
	issn = {1078-8956, 1546-170X},
	url = {https://www.nature.com/articles/s41591-023-02448-8},
	doi = {10.1038/s41591-023-02448-8},
	language = {en},
	number = {8},
	urldate = {2023-10-31},
	journal = {Nature Medicine},
	author = {Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
	month = aug,
	year = {2023},
	pages = {1930--1940},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Transactions on Machine Learning Research (TMLR), 2022},
	file = {arXiv Fulltext PDF:/Users/jedrzejkubica/Zotero/storage/JUI5NF4R/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/jedrzejkubica/Zotero/storage/UHF6A3K6/2206.html:text/html},
}

@misc{chang_survey_2023,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.03109},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = oct,
	year = {2023},
	note = {arXiv:2307.03109 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 31 pages; a major update to include more recent works; https://llm-eval.github.io/},
	file = {arXiv Fulltext PDF:/Users/jedrzejkubica/Zotero/storage/9SB98TNW/Chang et al. - 2023 - A Survey on Evaluation of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/jedrzejkubica/Zotero/storage/MARK9MAE/2307.html:text/html},
}

@misc{yasunaga_qa-gnn_2022,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	shorttitle = {{QA}-{GNN}},
	url = {http://arxiv.org/abs/2104.06378},
	abstract = {The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	month = dec,
	year = {2022},
	note = {arXiv:2104.06378 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: NAACL 2021. Code \& data available at https://github.com/michiyasunaga/qagnn},
	file = {arXiv Fulltext PDF:/Users/jedrzejkubica/Zotero/storage/HDYZFZZS/Yasunaga et al. - 2022 - QA-GNN Reasoning with Language Models and Knowled.pdf:application/pdf;arXiv.org Snapshot:/Users/jedrzejkubica/Zotero/storage/44GDMH26/2104.html:text/html},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/jedrzejkubica/Zotero/storage/AQFSGT24/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;arXiv.org Snapshot:/Users/jedrzejkubica/Zotero/storage/WQYYJAZ9/2307.html:text/html},
}

@article{zhao_po2rdf_2022,
	title = {{PO2RDF}: representation of real-world data for precision oncology using resource description framework},
	volume = {15},
	issn = {1755-8794},
	shorttitle = {{PO2RDF}},
	url = {https://bmcmedgenomics.biomedcentral.com/articles/10.1186/s12920-022-01314-9},
	doi = {10.1186/s12920-022-01314-9},
	abstract = {Abstract
            
              Background
              Next-generation sequencing provides comprehensive information about individuals’ genetic makeup and is commonplace in precision oncology practice. Due to the heterogeneity of individual patient’s disease conditions and treatment journeys, not all targeted therapies were initiated despite actionable mutations. To better understand and support the clinical decision-making process in precision oncology, there is a need to examine real-world associations between patients’ genetic information and treatment choices.
            
            
              Methods
              To fill the gap of insufficient use of real-world data (RWD) in electronic health records (EHRs), we generated a single Resource Description Framework (RDF) resource, called PO2RDF (precision oncology to RDF), by integrating information regarding genes, variants, diseases, and drugs from genetic reports and EHRs.
            
            
              Results
              There are a total 2,309,014 triples contained in the PO2RDF. Among them, 32,815 triples are related to Gene, 34,695 triples are related to Variant, 8,787 triples are related to Disease, 26,154 triples are related to Drug. We performed two use case analyses to demonstrate the usability of the PO2RDF: (1) we examined real-world associations between EGFR mutations and targeted therapies to confirm existing knowledge and detect off-label use. (2) We examined differences in prognosis for lung cancer patients with/without TP53 mutations.
            
            
              Conclusions
              In conclusion, our work proposed to use RDF to organize and distribute clinical RWD that is otherwise inaccessible externally. Our work serves as a pilot study that will lead to new clinical applications and could ultimately stimulate progress in the field of precision oncology.},
	language = {en},
	number = {1},
	urldate = {2023-10-31},
	journal = {BMC Medical Genomics},
	author = {Zhao, Yiqing and Dimou, Anastasios and Shen, Feichen and Zong, Nansu and Davila, Jaime I. and Liu, Hongfang and Wang, Chen},
	month = dec,
	year = {2022},
	pages = {167},
	file = {Full Text:/Users/jedrzejkubica/Zotero/storage/5CSP5AF9/Zhao et al. - 2022 - PO2RDF representation of real-world data for prec.pdf:application/pdf},
}

@article{vasaikar_proteogenomic_2019,
	title = {Proteogenomic {Analysis} of {Human} {Colon} {Cancer} {Reveals} {New} {Therapeutic} {Opportunities}},
	volume = {177},
	issn = {00928674},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867419302922},
	doi = {10.1016/j.cell.2019.03.030},
	language = {en},
	number = {4},
	urldate = {2023-10-31},
	journal = {Cell},
	author = {Vasaikar, Suhas and Huang, Chen and Wang, Xiaojing and Petyuk, Vladislav A. and Savage, Sara R. and Wen, Bo and Dou, Yongchao and Zhang, Yun and Shi, Zhiao and Arshad, Osama A. and Gritsenko, Marina A. and Zimmerman, Lisa J. and McDermott, Jason E. and Clauss, Therese R. and Moore, Ronald J. and Zhao, Rui and Monroe, Matthew E. and Wang, Yi-Ting and Chambers, Matthew C. and Slebos, Robbert J.C. and Lau, Ken S. and Mo, Qianxing and Ding, Li and Ellis, Matthew and Thiagarajan, Mathangi and Kinsinger, Christopher R. and Rodriguez, Henry and Smith, Richard D. and Rodland, Karin D. and Liebler, Daniel C. and Liu, Tao and Zhang, Bing and Pandey, Akhilesh and Paulovich, Amanda and Hoofnagle, Andrew and Mani, D.R. and Chan, Daniel W. and Ransohoff, David F. and Fenyo, David and Tabb, David L. and Levine, Douglas A. and Boja, Emily S. and Kuhn, Eric and White, Forest M. and Whiteley, Gordon A. and Zhu, Heng and Zhang, Hui and Shih, Ie-Ming and Bavarva, Jasmin and Whiteaker, Jeffrey and Ketchum, Karen A. and Clauser, Karl R. and Ruggles, Kelly and Elburn, Kimberly and Hannick, Linda and Watson, Mark and Oberti, Mauricio and Mesri, Mehdi and Sanders, Melinda E. and Borucki, Melissa and Gillette, Michael A. and Snyder, Michael and Edwards, Nathan J. and Vatanian, Negin and Rudnick, Paul A. and McGarvey, Peter B. and Mertins, Philip and Townsend, R. Reid and Thangudu, Ratna R. and Rivers, Robert C. and Payne, Samuel H. and Davies, Sherri R. and Cai, Shuang and Stein, Stephen E. and Carr, Steven A. and Skates, Steven J. and Madhavan, Subha and Hiltke, Tara and Chen, Xian and Zhao, Yingming and Wang, Yue and Zhang, Zhen},
	month = may,
	year = {2019},
	pages = {1035--1049.e19},
	file = {Full Text:/Users/jedrzejkubica/Zotero/storage/BJEZJZ4Y/Vasaikar et al. - 2019 - Proteogenomic Analysis of Human Colon Cancer Revea.pdf:application/pdf},
}

@article{zhou_ttd_2023,
	title = {{TTD}: \textit{{Therapeutic} {Target} {Database}} describing target druggability information},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{TTD}},
	url = {https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkad751/7275004},
	doi = {10.1093/nar/gkad751},
	abstract = {Abstract
            Target discovery is one of the essential steps in modern drug development, and the identification of promising targets is fundamental for developing first-in-class drug. A variety of methods have emerged for target assessment based on druggability analysis, which refers to the likelihood of a target being effectively modulated by drug-like agents. In the therapeutic target database (TTD), nine categories of established druggability characteristics were thus collected for 426 successful, 1014 clinical trial, 212 preclinical/patented, and 1479 literature-reported targets via systematic review. These characteristic categories were classified into three distinct perspectives: molecular interaction/regulation, human system profile and cell-based expression variation. With the rapid progression of technology and concerted effort in drug discovery, TTD and other databases were highly expected to facilitate the explorations of druggability characteristics for the discovery and validation of innovative drug target. TTD is now freely accessible at: https://idrblab.org/ttd/.},
	language = {en},
	urldate = {2023-10-31},
	journal = {Nucleic Acids Research},
	author = {Zhou, Ying and Zhang, Yintao and Zhao, Donghai and Yu, Xinyuan and Shen, Xinyi and Zhou, Yuan and Wang, Shanshan and Qiu, Yunqing and Chen, Yuzong and Zhu, Feng},
	month = sep,
	year = {2023},
	pages = {gkad751},
	file = {Full Text:/Users/jedrzejkubica/Zotero/storage/RJ8S7IA5/Zhou et al. - 2023 - TTD Therapeutic Target Database describing.pdf:application/pdf},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Extended NeurIPS submission},
	file = {arXiv Fulltext PDF:/Users/jedrzejkubica/Zotero/storage/IDBM2T9Z/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf;arXiv.org Snapshot:/Users/jedrzejkubica/Zotero/storage/2AGR7BNL/2305.html:text/html},
}

@misc{noauthor_openai_nodate,
	title = {{OpenAI}. (2023). {ChatGPT} ({Feb} 13 version) [{Large} language model]. https://chat.openai.com},
}

@article{wang_text_2021,
	title = {Text mining approaches for dealing with the rapidly expanding literature on {COVID}-19},
	volume = {22},
	issn = {1467-5463, 1477-4054},
	url = {https://academic.oup.com/bib/article/22/2/781/6024738},
	doi = {10.1093/bib/bbaa296},
	abstract = {Abstract More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a qualitative description and assessment of the system’s performance, unique data or user interface features and modeling decisions. Many systems focus on search and discovery, though several systems provide novel features, such as the ability to summarize findings over multiple documents or linking between scientific articles and clinical trials. We also describe the public corpora, models and shared tasks that have been introduced to help reduce repeated effort among community members; some of these resources (especially shared tasks) can provide a basis for comparing the performance of different systems. Finally, we summarize promising results and open challenges for text mining the COVID-19 literature.},
	language = {en},
	number = {2},
	urldate = {2023-10-31},
	journal = {Briefings in Bioinformatics},
	author = {Wang, Lucy Lu and Lo, Kyle},
	month = mar,
	year = {2021},
	pages = {781--799},
	file = {Full Text:/Users/jedrzejkubica/Zotero/storage/Z8WKDGG9/Wang and Lo - 2021 - Text mining approaches for dealing with the rapidl.pdf:application/pdf},
}
